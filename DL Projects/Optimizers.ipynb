{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sat_sum</th>\n",
       "      <th>hs_gpa</th>\n",
       "      <th>fy_gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>464</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>428</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sat_sum  hs_gpa  fy_gpa\n",
       "0      508    3.40    3.18\n",
       "1      488    4.00    3.33\n",
       "2      464    3.75    3.25\n",
       "3      380    3.75    2.42\n",
       "4      428    4.00    2.63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Prodigy University Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"sat_sum\",\"hs_gpa\"]].values\n",
    "y = df[\"fy_gpa\"].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype= torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3856, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = nn.Sequential(nn.Linear(2,2),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(2,1))\n",
    "    return model\n",
    "model = create_model()\n",
    "criteria = nn.MSELoss()\n",
    "preds = model(X_train_tensor)\n",
    "initial_loss = criteria(preds,y_train_tensor)\n",
    "print(initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1314, -0.2272]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight\n",
    "model[2].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single epoch of SGD optimizer manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3356, -0.4660]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.001)\n",
    "initial_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "model[0].weight\n",
    "model[2].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without training: \n",
      " Train Loss : 5.8944 Test Loss : 6.1484\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "## performance of train and test before model training\n",
    "train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "print(f\"Model without training: \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \n",
      " Train Loss : 0.5482 Test Loss : 0.6130\n",
      "Epoch : 1 \n",
      " Train Loss : 0.4784 Test Loss : 0.5306\n",
      "Epoch : 2 \n",
      " Train Loss : 0.4395 Test Loss : 0.4888\n",
      "Epoch : 3 \n",
      " Train Loss : 0.4119 Test Loss : 0.4622\n",
      "Epoch : 4 \n",
      " Train Loss : 0.3919 Test Loss : 0.4404\n",
      "Epoch : 5 \n",
      " Train Loss : 0.3785 Test Loss : 0.4284\n",
      "Epoch : 6 \n",
      " Train Loss : 0.3692 Test Loss : 0.4176\n",
      "Epoch : 7 \n",
      " Train Loss : 0.3634 Test Loss : 0.4114\n",
      "Epoch : 8 \n",
      " Train Loss : 0.3586 Test Loss : 0.4084\n",
      "Epoch : 9 \n",
      " Train Loss : 0.3557 Test Loss : 0.4081\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=1, shuffle = True)\n",
    "for epoch in range(10):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    ## performance of train and test before model training\n",
    "    train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "    test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "    print(f\"Epoch : {epoch} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 \n",
      " Train Loss : 5.2699 Test Loss : 5.5114\n",
      "Epoch : 100 \n",
      " Train Loss : 3.1497 Test Loss : 3.3442\n",
      "Epoch : 200 \n",
      " Train Loss : 2.0168 Test Loss : 2.1771\n",
      "Epoch : 300 \n",
      " Train Loss : 1.4075 Test Loss : 1.5427\n",
      "Epoch : 400 \n",
      " Train Loss : 1.0785 Test Loss : 1.1953\n",
      "Epoch : 500 \n",
      " Train Loss : 0.8998 Test Loss : 1.0029\n",
      "Epoch : 600 \n",
      " Train Loss : 0.8014 Test Loss : 0.8945\n",
      "Epoch : 700 \n",
      " Train Loss : 0.7460 Test Loss : 0.8316\n",
      "Epoch : 800 \n",
      " Train Loss : 0.7135 Test Loss : 0.7936\n",
      "Epoch : 900 \n",
      " Train Loss : 0.6932 Test Loss : 0.7692\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "train_loader = DataLoader(train_data, batch_size = 800, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini batch gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 \n",
      " Train Loss : 0.5167 Test Loss : 0.5904\n",
      "Epoch : 100 \n",
      " Train Loss : 0.4342 Test Loss : 0.4893\n",
      "Epoch : 150 \n",
      " Train Loss : 0.4112 Test Loss : 0.4639\n",
      "Epoch : 200 \n",
      " Train Loss : 0.3947 Test Loss : 0.4473\n",
      "Epoch : 250 \n",
      " Train Loss : 0.3826 Test Loss : 0.4354\n",
      "Epoch : 300 \n",
      " Train Loss : 0.3737 Test Loss : 0.4268\n",
      "Epoch : 350 \n",
      " Train Loss : 0.3670 Test Loss : 0.4206\n",
      "Epoch : 400 \n",
      " Train Loss : 0.3621 Test Loss : 0.4162\n",
      "Epoch : 450 \n",
      " Train Loss : 0.3585 Test Loss : 0.4128\n",
      "Epoch : 500 \n",
      " Train Loss : 0.3557 Test Loss : 0.4104\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(500):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with momentum\n",
    "Approach : Finding moving average of gradients\n",
    "* Accelerates SGD\n",
    "* Dampens the turbulence\n",
    "* Special used when a loss function has multiple local minima but one global minima\n",
    "* Helps in arriving global minima\n",
    "* w_new = w_old - learning_rate * exponential moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 \n",
      " Train Loss : 0.3579 Test Loss : 0.4170\n",
      "Epoch : 100 \n",
      " Train Loss : 0.3483 Test Loss : 0.4077\n",
      "Epoch : 150 \n",
      " Train Loss : 0.3463 Test Loss : 0.4054\n",
      "Epoch : 200 \n",
      " Train Loss : 0.3451 Test Loss : 0.4042\n",
      "Epoch : 250 \n",
      " Train Loss : 0.3443 Test Loss : 0.4032\n",
      "Epoch : 300 \n",
      " Train Loss : 0.3436 Test Loss : 0.4026\n",
      "Epoch : 350 \n",
      " Train Loss : 0.3431 Test Loss : 0.4025\n",
      "Epoch : 400 \n",
      " Train Loss : 0.3426 Test Loss : 0.4023\n",
      "Epoch : 450 \n",
      " Train Loss : 0.3423 Test Loss : 0.4019\n",
      "Epoch : 500 \n",
      " Train Loss : 0.3420 Test Loss : 0.4021\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "# Added Momentum of 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(500):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum\n",
    "\n",
    "Refines momentum by looking at gradient of future steps\n",
    "Set nestrov = True in optimizer along with momentum of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 \n",
      " Train Loss : 0.3619 Test Loss : 0.4183\n",
      "Epoch : 100 \n",
      " Train Loss : 0.3493 Test Loss : 0.4071\n",
      "Epoch : 150 \n",
      " Train Loss : 0.3470 Test Loss : 0.4051\n",
      "Epoch : 200 \n",
      " Train Loss : 0.3457 Test Loss : 0.4039\n",
      "Epoch : 250 \n",
      " Train Loss : 0.3448 Test Loss : 0.4028\n",
      "Epoch : 300 \n",
      " Train Loss : 0.3440 Test Loss : 0.4027\n",
      "Epoch : 350 \n",
      " Train Loss : 0.3434 Test Loss : 0.4019\n",
      "Epoch : 400 \n",
      " Train Loss : 0.3429 Test Loss : 0.4015\n",
      "Epoch : 450 \n",
      " Train Loss : 0.3424 Test Loss : 0.4013\n",
      "Epoch : 500 \n",
      " Train Loss : 0.3421 Test Loss : 0.4011\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9, nesterov=True)\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(500):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad : \n",
    "Adaptive Gradient Descent uses different learning rates for each iteration in updating weights based on historical gradients.\n",
    "* Parameters with infrequent updates --> Bigger updates\n",
    "* Parameters with frequent updates --> Smaller updates\n",
    "* Used in Sparse Datasets like Image and text datasets\n",
    "* Problem: It my reduce learning rates aggressively resulting in sub-optimal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 \n",
      " Train Loss : 0.8502 Test Loss : 0.9670\n",
      "Epoch : 100 \n",
      " Train Loss : 0.4995 Test Loss : 0.5887\n",
      "Epoch : 150 \n",
      " Train Loss : 0.3973 Test Loss : 0.4714\n",
      "Epoch : 200 \n",
      " Train Loss : 0.3662 Test Loss : 0.4319\n",
      "Epoch : 250 \n",
      " Train Loss : 0.3564 Test Loss : 0.4176\n",
      "Epoch : 300 \n",
      " Train Loss : 0.3531 Test Loss : 0.4120\n",
      "Epoch : 350 \n",
      " Train Loss : 0.3518 Test Loss : 0.4094\n",
      "Epoch : 400 \n",
      " Train Loss : 0.3512 Test Loss : 0.4081\n",
      "Epoch : 450 \n",
      " Train Loss : 0.3509 Test Loss : 0.4074\n",
      "Epoch : 500 \n",
      " Train Loss : 0.3506 Test Loss : 0.4071\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "# Added Momentum of 0.9\n",
    "optimizer = optim.Adagrad(model.parameters())\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(500):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp- Root Mean Square Propagation\n",
    "Accelerates the optimization process by reducing the number of updates needed to reach the minima. jUST like Momentum. But, if a parameter loss function oscillates the prediction we should penalize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 \n",
      " Train Loss : 0.3420 Test Loss : 0.4021\n",
      "Epoch : 100 \n",
      " Train Loss : 0.3402 Test Loss : 0.3992\n",
      "Epoch : 150 \n",
      " Train Loss : 0.3397 Test Loss : 0.4012\n",
      "Epoch : 200 \n",
      " Train Loss : 0.3404 Test Loss : 0.3994\n",
      "Epoch : 250 \n",
      " Train Loss : 0.3389 Test Loss : 0.3999\n",
      "Epoch : 300 \n",
      " Train Loss : 0.3396 Test Loss : 0.4038\n",
      "Epoch : 350 \n",
      " Train Loss : 0.3388 Test Loss : 0.4006\n",
      "Epoch : 400 \n",
      " Train Loss : 0.3387 Test Loss : 0.4005\n",
      "Epoch : 450 \n",
      " Train Loss : 0.3387 Test Loss : 0.4017\n",
      "Epoch : 500 \n",
      " Train Loss : 0.3390 Test Loss : 0.4000\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "# Added Momentum of 0.9\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(500):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM Optimizer\n",
    "Adaptive movement Estimation is a combination of RMSProp and Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50 \n",
      " Train Loss : 0.9142 Test Loss : 1.0298\n",
      "Epoch : 100 \n",
      " Train Loss : 0.3704 Test Loss : 0.4317\n",
      "Epoch : 150 \n",
      " Train Loss : 0.3569 Test Loss : 0.4103\n",
      "Epoch : 200 \n",
      " Train Loss : 0.3554 Test Loss : 0.4090\n",
      "Epoch : 250 \n",
      " Train Loss : 0.3536 Test Loss : 0.4078\n",
      "Epoch : 300 \n",
      " Train Loss : 0.3517 Test Loss : 0.4063\n",
      "Epoch : 350 \n",
      " Train Loss : 0.3496 Test Loss : 0.4054\n",
      "Epoch : 400 \n",
      " Train Loss : 0.3477 Test Loss : 0.4044\n",
      "Epoch : 450 \n",
      " Train Loss : 0.3461 Test Loss : 0.4031\n",
      "Epoch : 500 \n",
      " Train Loss : 0.3448 Test Loss : 0.4027\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "model = create_model()\n",
    "# Added Momentum of 0.9\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)#800 is no of train samples\n",
    "\n",
    "for epoch in range(500):\n",
    "    for X_batch,y_batch in train_loader:\n",
    "        #forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criteria(pred,y_batch)\n",
    "        \n",
    "        #backward pass and optimization\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        ## performance of train and test before model training\n",
    "        train_loss = criteria(model(X_train_tensor), y_train_tensor)\n",
    "        test_loss = criteria(model(X_test_tensor),y_test_tensor)\n",
    "        print(f\"Epoch : {epoch+1} \\n Train Loss : {train_loss:0.4f} Test Loss : {test_loss:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
